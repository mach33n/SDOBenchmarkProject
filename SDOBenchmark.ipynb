{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c05422",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3561b461-68c8-4779-a83f-eb1cb4520560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageChops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67de96-b138-41df-9fcd-3b60cc127536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(glob.glob(\"./training/11390/2012_01_05_17_06_01_0/*\"))) + \" Images for sample 1\")\n",
    "print(str(len(glob.glob(\"./training/*/*\"))) + \" Different Samples\")\n",
    "# Avg number of images per sample\n",
    "print(str(len(glob.glob(\"./training/*/*/*\"))/len(glob.glob(\"./training/*/*\"))) + \" Average number images per sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fec3f7-97c8-4d2f-addd-4ab18bd1416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for a single sample\n",
    "sample = []\n",
    "for i in glob.glob(\"./training/11390/2012_01_05_17_06_01_0/*\"):\n",
    "    print(i)\n",
    "    sample.append(Image.open(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89362f22-8024-4b72-905b-ab31e705f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob(\"./training/11390/2012_01_05_17_06_01_0/*_continuum.jpg\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad49a96-3df1-4a8e-ac90-158050fdf920",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob(\"./training/11390/2012_01_05_17_06_01_0/*_magnetogram.jpg\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67364843-2499-4858-9ddd-2931839f2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob(\"./training/11390/2012_01_05_17_06_01_0/*_211.jpg\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca01c0c-cf05-46ed-81ae-157774939034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _# represents AIA wavelength for band #\n",
    "# Hour Times: 05, 12, 15, 16\n",
    "# Not sure what continuum images represent\n",
    "for i in glob.glob(\"./training/11390/2012_01_05_17_06_01_0/*_304.jpg\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654af0e-8183-4087-9e7f-88e81e75e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e767e5e-cf76-45fc-98e5-9661ebbcf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = sample[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c509a15-d5f5-4293-81c1-d1e4085faaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"./training/11390/2012_01_05_17_06_01_0/2012-01-05T153601__magnetogram.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedc65d-4e28-4e75-8245-fcbd36720f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.format)\n",
    "print(img.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e766d-c02a-4180-ab74-deb3930fbd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af8d027-3c19-4163-91ad-f298267a34c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8E-07 is peak flux for this sample\n",
    "# Data Transformation 1: \n",
    "i = img.split()[0]\n",
    "len(i.histogram())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349bbea9",
   "metadata": {},
   "source": [
    "## Baseline network\n",
    "This network should directly predict peak flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ad1415-ba73-4bd6-b2f7-db7242cbe986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv3d(4, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (conv2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (conv3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (pool3d): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=65536, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Adjusted convolutional layers for 3D data\n",
    "        self.conv1 = nn.Conv3d(in_channels=4, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv3d(32, 64, 3, padding=1)\n",
    "\n",
    "        # Adjusted pooling layers\n",
    "        self.pool3d = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 32 * 32 * 1, 512)\n",
    "        \n",
    "        # Rest of the network remains the same\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add sequence of convolutional and max pooling layers\n",
    "        x = self.pool3d(F.relu(self.conv1(x)))\n",
    "        x = self.pool3d(F.relu(self.conv2(x)))\n",
    "        x = self.pool3d(F.relu(self.conv3(x)))\n",
    "\n",
    "        # Flatten image input\n",
    "        x = x.view(-1, 64 * 32 * 32 * 1)  # Update these dimensions accordingly\n",
    "\n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Add 2nd hidden layer for regression\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the CNN for regression\n",
    "model = SimpleCNN()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Model\n",
    "This model predicts temperature and then converts at the end to flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNWithWien(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNNWithWien, self).__init__()\n",
    "        # Adjusted convolutional layers for 3D data\n",
    "        self.conv1 = nn.Conv3d(in_channels=4, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv3d(32, 64, 3, padding=1)\n",
    "\n",
    "        # Adjusted pooling layers\n",
    "        self.pool3d = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "        # Linear layers\n",
    "        # Update the input features of fc1 according to the output size of the last pooling layer\n",
    "        # Example dimensions: 64 * 32 * 32 * 1 = 65536, if the final output size is [64, 32, 32, 1]\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 512)  # Adjust these dimensions\n",
    "        self.fc2 = nn.Linear(512, 1)  # Outputting temperature\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # Wien's displacement constant (meters * Kelvin)\n",
    "        self.wien_constant = 2.897771955e-3\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add sequence of convolutional and max pooling layers\n",
    "        x = self.pool3d(F.relu(self.conv1(x)))\n",
    "        x = self.pool3d(F.relu(self.conv2(x)))\n",
    "        x = self.pool3d(F.relu(self.conv3(x)))\n",
    "\n",
    "        # Flatten image input\n",
    "        x = x.view(-1, 64 * 32 * 32)  # Update these dimensions accordingly\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Add 2nd hidden layer (outputs temperature)\n",
    "        temperature = self.fc2(x)\n",
    "\n",
    "        # Apply Wien's equation to calculate peak wavelength\n",
    "        # Prevent division by zero in case temperature is zero\n",
    "        temperature = torch.clamp(temperature, min=1e-6)\n",
    "        peak_wavelength = self.wien_constant / temperature\n",
    "\n",
    "        return peak_wavelength\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e4e96b",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "A single input will be a 4x256x256x10 matrix where dimensions represent: [timeinterval x height x width x wavelength/magnetogram]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 21,
>>>>>>> 33a0d65d57 (faster)
   "id": "314188a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Get a list of all active region numbers\n",
    "all_files = glob.glob(\"./training/*/*\")\n",
    "\n",
    "# Split all_files into 10 chunks\n",
    "chunks = np.array_split(all_files, 40)\n",
    "print(len(chunks[0]))\n",
    "\n",
    "def generateChunk(idxs):\n",
    "    wavelengths = [\"94\",\"131\", \"171\",\"193\",\"211\",\"304\",\"335\",\"1700\",\"continuum\",\"magnetogram\"]\n",
    "    x = np.zeros((len(idxs), 4, 256, 256, 10), dtype=np.float32)\n",
    "    y = np.zeros((len(idxs), 1), dtype=np.float32)\n",
    "    df = pd.read_csv('training/meta_data.csv')\n",
    "\n",
    "    for i, sample in enumerate(tqdm(idxs)):\n",
    "        images = np.empty((4, 256, 256, 10), dtype=np.int64)\n",
    "        for j, wave in enumerate(wavelengths):\n",
    "            path = sample + \"/*_{}.jpg\".format(wave)\n",
    "            pics = np.array([np.array(Image.open(i)) for i in glob.glob(path)])\n",
    "            # Create zero images\n",
    "            zero_images = [np.zeros((256, 256), dtype=np.int64) for _ in range(4 - len(pics))]\n",
    "\n",
    "            # Add zero images to the list of images\n",
    "            pics = np.array(list(pics) + zero_images)\n",
    "\n",
    "            # Now, you can assign pics to images\n",
    "            images[:, :, :, j] = pics \n",
    "        images = images[:, :, :, :]\n",
    "        x[i] = images\n",
    "\n",
    "        idx = path.split(\"/\")[2] + \"_\"+ path.split(\"/\")[3]\n",
    "        y[i] = df[df[\"id\"] == idx]['peak_flux'].iloc[0]\n",
    "\n",
    "    return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "#for i in range(40):\n",
    "#    train_x, train_y = generateChunk(chunks[i])\n",
    "#    dataset = TensorDataset(train_x, train_y)\n",
    "#    torch.save(dataset, \"datachunks/chunk_{}.pt\".format(i))\n",
    "\n",
    "## Create a DataLoader\n",
    "#val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 22,
>>>>>>> 33a0d65d57 (faster)
   "id": "685d339d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "  0%|          | 0/209 [00:00<?, ?it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file './training/12652/2017_04_23_22_19_01_0/2017-04-23T171901__94.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mavg_loss\u001b[39m}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m mod \u001b[39m=\u001b[39m SimpleCNN()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m train_model(mod, nn\u001b[39m.\u001b[39;49mMSELoss(), torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(mod\u001b[39m.\u001b[39;49mparameters()), \u001b[39m10\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m mod \u001b[39m=\u001b[39m SimpleCNNWithWien()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m train_model(mod, nn\u001b[39m.\u001b[39mMSELoss(), torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(mod\u001b[39m.\u001b[39mparameters()), \u001b[39m10\u001b[39m)\n",
      "\u001b[1;32m/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m39\u001b[39m)):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         train_x, train_y \u001b[39m=\u001b[39m generateChunk(chunks[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         dataset \u001b[39m=\u001b[39m TensorDataset(train_x, train_y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         train_loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m wave \u001b[39min\u001b[39;00m wavelengths:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     path \u001b[39m=\u001b[39m sample \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/*_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(wave)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     pics \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([np\u001b[39m.\u001b[39marray(Image\u001b[39m.\u001b[39mopen(i)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m glob\u001b[39m.\u001b[39mglob(path)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(pics)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         pics \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((pics, torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, \u001b[39m256\u001b[39m,\u001b[39m256\u001b[39m))), \u001b[39m0\u001b[39m)\n",
      "\u001b[1;32m/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m wave \u001b[39min\u001b[39;00m wavelengths:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     path \u001b[39m=\u001b[39m sample \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/*_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(wave)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     pics \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([np\u001b[39m.\u001b[39marray(Image\u001b[39m.\u001b[39;49mopen(i)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m glob\u001b[39m.\u001b[39mglob(path)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(pics)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cameron/SDOBenchmarkProject/SDOBenchmark.ipynb#X26sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         pics \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((pics, torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, \u001b[39m256\u001b[39m,\u001b[39m256\u001b[39m))), \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:3305\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3303\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message)\n\u001b[1;32m   3304\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcannot identify image file \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (filename \u001b[39mif\u001b[39;00m filename \u001b[39melse\u001b[39;00m fp)\n\u001b[0;32m-> 3305\u001b[0m \u001b[39mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file './training/12652/2017_04_23_22_19_01_0/2017-04-23T171901__94.jpg'"
=======
      "100%|██████████| 208/208 [00:06<00:00, 32.68it/s]\n",
      "100%|██████████| 209/209 [00:07<00:00, 29.54it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.93it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 31.52it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.74it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.37it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.52it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.39it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.45it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.53it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.21it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.41it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.09it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.41it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.43it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.59it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.47it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.42it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.49it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.86it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.34it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 30.65it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.36it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.13it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.71it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.37it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.52it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.22it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.50it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.65it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.57it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.44it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.19it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.33it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.26it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.14it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.52it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.61it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 33.11it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 32.37it/s]\n",
      "100%|██████████| 39/39 [46:14<00:00, 71.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 2003.8599, Train Acc: 0.0000, Val Loss: 0.0121, Val Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:06<00:00, 32.54it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.28it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.60it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.61it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.73it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.47it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.33it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.19it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.65it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.34it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.01it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.06it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 33.41it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 32.91it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 33.72it/s]\n",
      "100%|██████████| 209/209 [00:06<00:00, 33.83it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 33.45it/s]\n",
      "100%|██████████| 208/208 [00:06<00:00, 33.24it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.96it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.60it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.42it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.66it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.47it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.21it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.07it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 23.96it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.20it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.00it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.27it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 23.89it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 23.89it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 23.80it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 23.82it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.63it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.45it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.60it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.73it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 25.25it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.64it/s]\n",
      "100%|██████████| 39/39 [1:01:30<00:00, 94.63s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.0005, Train Acc: 0.0000, Val Loss: 0.0006, Val Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:08<00:00, 24.27it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.26it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.47it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.48it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.51it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.37it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.25it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.34it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.51it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.17it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.49it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.35it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.45it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.30it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.43it/s]\n",
      "100%|██████████| 209/209 [00:08<00:00, 24.59it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.47it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.49it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 25.07it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.35it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.39it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.48it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.31it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.68it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.25it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.40it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.67it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.55it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.73it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 24.39it/s]\n",
      "100%|██████████| 208/208 [00:08<00:00, 23.93it/s]\n"
>>>>>>> 33a0d65d57 (faster)
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "def train_model(model, criterion, optimizer, epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move the model to the GPU if available\n",
=======
    "import torch.cuda as cuda\n",
    "\n",
    "def train_model(model, criterion, optimizer, epochs):\n",
    "    device = torch.device(\"cuda\" if cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move the model to GPU if available\n",
    "\n",
    "    train_x, train_y = generateChunk(chunks[39])\n",
    "    dataset = TensorDataset(train_x, train_y)\n",
    "    val_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
>>>>>>> 33a0d65d57 (faster)
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for i in tqdm(range(39)):\n",
    "            train_x, train_y = generateChunk(chunks[i])\n",
    "            dataset = TensorDataset(train_x, train_y)\n",
    "            train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "            for inputs, targets in train_loader:\n",
<<<<<<< HEAD
    "                inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n",
=======
    "                inputs = inputs.to(device)  # Move the data to GPU if available\n",
    "                targets = targets.to(device)  # Move the data to GPU if available\n",
    "\n",
>>>>>>> 33a0d65d57 (faster)
    "                optimizer.zero_grad()  # Reset the gradients\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "                loss = criterion(outputs, targets.float())  # Compute loss\n",
    "                loss.backward()  # Backward pass\n",
    "                optimizer.step()  # Update weights\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                total_samples += inputs.size(0)\n",
    "                correct_predictions += torch.sum(outputs == targets).item()\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # No need to track gradients in validation phase\n",
<<<<<<< HEAD
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_loss += loss.item() * len(targets)\n",
    "                total_correct += torch.sum(outputs == targets)\n",
    "                total_samples += len(targets)\n",
    "        \n",
    "        avg_loss = total_loss / total_samples\n",
    "        accuracy = total_correct / total_samples\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss}, Accuracy: {accuracy}')\n",
=======
    "            val_loss = 0.0\n",
    "            val_samples = 0\n",
    "            val_correct_predictions = 0\n",
    "\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.to(device)  # Move the data to GPU if available\n",
    "                targets = targets.to(device)  # Move the data to GPU if available\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_samples += inputs.size(0)\n",
    "                val_correct_predictions += torch.sum(outputs == targets).item()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = correct_predictions / total_samples\n",
    "        val_epoch_loss = val_loss / val_samples\n",
    "        val_epoch_acc = val_correct_predictions / val_samples\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}')\n",
>>>>>>> 33a0d65d57 (faster)
    "\n",
    "mod = SimpleCNN()\n",
    "train_model(mod, nn.MSELoss(), torch.optim.Adam(mod.parameters()), 10)\n",
    "mod = SimpleCNNWithWien()\n",
    "train_model(mod, nn.MSELoss(), torch.optim.Adam(mod.parameters()), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
